# This is a sample config file for {7m1t}x3 Long-LRM with token merging at 9th layer. 

# general settings
checkpoint_dir: "checkpoints"
evaluation_dir: "evaluation"
api_key_path: "./api_keys.yaml"
use_tf32: true
use_amp: true
amp_dtype: "bf16"

model:
  num_layers: 24
  patch_size: 8
  dim: [256, 1024]
  block_type: "mmmmmmmt"
  merge_layers: [8]
  transformer:
    head_dim: 64
  mamba2:
    d_state: 256

  num_global_tokens: 2
  gaussians:
    sh_degree: 3
    near_plane: 0.01
    far_plane: 1000000.0
    scale_bias: -6.9
    scale_max: -1.2
    opacity_bias: -2.0
    align_to_pixel: true
    max_dist: 500.0

training:
  # optimizer
  lr: 0.0004
  beta1: 0.9
  beta2: 0.95
  weight_decay: 0.05
  warmup_steps: 2000
  scheduler_type: "cosine"
  grad_accum_steps: 1
  grad_clip_norm: 10.0

  # dataloader
  # 4
  num_workers: 4
  prefetch_factor: 32

  # wandb
  wandb_offline: false
  wandb_entity: aman190202-brown-university
  wandb_project:  long-lrm

  # losses
  l2_loss: 1.0
  perceptual_loss: 1.0
  # perceptual loss settings from https://github.com/zhengqili/Crowdsampling-the-Plenoptic-Function/blob/master/models/networks.py#L1478
  perceptual_out_idx: [4, 9, 14, 23, 32]
  perceptual_out_weights: [0.3846, 0.2083, 0.2703, 0.1786, 6.6667]
  perceptual_feature_scale: 255.0
  # weights from https://www.vlfeat.org/matconvnet/pretrained/
  perceptual_vgg_weights: "model/matconvnet_vgg19.pt"
  opacity_loss: 0.1
  gaussian_depth_loss: 0.01

  # logging
  print_every: 10
  wandb_every: 10
  checkpoint_every: 500
  vis_every: 100
  save_gaussian_every: 500
  save_video_every: 500